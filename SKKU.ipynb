{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0f1f37f7-d4f9-4d75-a10f-36a68ee6dc51",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from vllm import LLM, SamplingParams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6a29360b-f9ad-48ed-a517-53b50f326dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì‚¬ìš©í•  ëª¨ë¸ ì‹ë³„ì ì •ì˜ (Hugging Face Model ID)\n",
    "# Qwen3-30B-A3B-FP8: MoE ì•„í‚¤í…ì²˜ ê¸°ë°˜ì´ë©°, FP8(8-bit Floating Point)ë¡œ ì–‘ìí™”ëœ ëª¨ë¸\n",
    "model_name = \"Qwen/Qwen3-30B-A3B-FP8\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2ceaf99d-27bf-4562-99cd-acdc34417a31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. í† í¬ë‚˜ì´ì € ì´ˆê¸°í™” (Tokenizer Initialization)\n",
    "# AutoTokenizer: ì…ë ¥ í…ìŠ¤íŠ¸(String)ë¥¼ ëª¨ë¸ì´ ì²˜ë¦¬ ê°€ëŠ¥í•œ ìˆ˜ì¹˜ ë°ì´í„°ì¸ 'Token ID(Integer)'ë¡œ ë³€í™˜í•˜ëŠ” ì „ì²˜ë¦¬ ëª¨ë“ˆì…ë‹ˆë‹¤.\n",
    "# - ì—­í• : ëª¨ë¸ì˜ ê³ ìœ  ì–´íœ˜ ì§‘í•©(Vocabulary)ì— ë§¤í•‘ëœ ì¸ë±ìŠ¤ë¥¼ ë°˜í™˜í•˜ë©°, ëª¨ë¸ì€ ì´ ì¸ë±ìŠ¤ë¥¼ í†µí•´ ì„ë² ë”© ë ˆì´ì–´(Embedding Layer)ì˜ ë²¡í„° ê°’ì„ ì°¸ì¡°í•©ë‹ˆë‹¤.\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dab118ab-546c-4202-8fe9-255f37063994",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 12-02 09:59:08 [utils.py:253] non-default args: {'gpu_memory_utilization': 0.95, 'disable_log_stats': True, 'model': 'Qwen/Qwen3-30B-A3B-FP8'}\n",
      "INFO 12-02 09:59:09 [model.py:631] Resolved architecture: Qwen3MoeForCausalLM\n",
      "INFO 12-02 09:59:09 [model.py:1745] Using max model len 40960\n",
      "INFO 12-02 09:59:10 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
      "\u001b[1;36m(EngineCore_DP0 pid=28616)\u001b[0;0m INFO 12-02 09:59:11 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='Qwen/Qwen3-30B-A3B-FP8', speculative_config=None, tokenizer='Qwen/Qwen3-30B-A3B-FP8', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=40960, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=fp8, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=Qwen/Qwen3-30B-A3B-FP8, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none', '+quant_fp8'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}\n",
      "\u001b[1;36m(EngineCore_DP0 pid=28616)\u001b[0;0m INFO 12-02 09:59:11 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://172.17.0.2:42183 backend=nccl\n",
      "\u001b[1;36m(EngineCore_DP0 pid=28616)\u001b[0;0m INFO 12-02 09:59:11 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "\u001b[1;36m(EngineCore_DP0 pid=28616)\u001b[0;0m INFO 12-02 09:59:12 [gpu_model_runner.py:3259] Starting to load model Qwen/Qwen3-30B-A3B-FP8...\n",
      "\u001b[1;36m(EngineCore_DP0 pid=28616)\u001b[0;0m INFO 12-02 09:59:12 [cuda.py:418] Valid backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']\n",
      "\u001b[1;36m(EngineCore_DP0 pid=28616)\u001b[0;0m INFO 12-02 09:59:12 [cuda.py:427] Using FLASH_ATTN backend.\n",
      "\u001b[1;36m(EngineCore_DP0 pid=28616)\u001b[0;0m INFO 12-02 09:59:12 [layer.py:342] Enabled separate cuda stream for MoE shared_experts\n",
      "\u001b[1;36m(EngineCore_DP0 pid=28616)\u001b[0;0m WARNING 12-02 09:59:12 [fp8.py:165] DeepGEMM backend requested but not available.\n",
      "\u001b[1;36m(EngineCore_DP0 pid=28616)\u001b[0;0m INFO 12-02 09:59:12 [fp8.py:180] Using Triton backend for FP8 MoE\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4810f1ec189144d5b78be7b1450c1aac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/7 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(EngineCore_DP0 pid=28616)\u001b[0;0m INFO 12-02 09:59:24 [default_loader.py:314] Loading weights took 11.04 seconds\n",
      "\u001b[1;36m(EngineCore_DP0 pid=28616)\u001b[0;0m INFO 12-02 09:59:25 [gpu_model_runner.py:3338] Model loading took 29.0445 GiB memory and 12.163751 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(EngineCore_DP0 pid=28616)\u001b[0;0m /home/jovyan/work/skku_venv/lib/python3.12/site-packages/torch/_dynamo/variables/functions.py:1692: UserWarning: Dynamo detected a call to a `functools.lru_cache`-wrapped function. Dynamo ignores the cache wrapper and directly traces the wrapped function. Silent incorrectness is only a *potential* risk, not something we have observed. Enable TORCH_LOGS=\"+dynamo\" for a DEBUG stack trace.\n",
      "\u001b[1;36m(EngineCore_DP0 pid=28616)\u001b[0;0m   torch._dynamo.utils.warn_once(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(EngineCore_DP0 pid=28616)\u001b[0;0m INFO 12-02 09:59:37 [backends.py:631] Using cache directory: /home/jovyan/.cache/vllm/torch_compile_cache/b003b827d1/rank_0_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(EngineCore_DP0 pid=28616)\u001b[0;0m INFO 12-02 09:59:37 [backends.py:647] Dynamo bytecode transform time: 12.57 s\n",
      "\u001b[1;36m(EngineCore_DP0 pid=28616)\u001b[0;0m INFO 12-02 09:59:43 [backends.py:210] Directly load the compiled graph(s) for dynamic shape from the cache, took 4.581 s\n",
      "\u001b[1;36m(EngineCore_DP0 pid=28616)\u001b[0;0m WARNING 12-02 09:59:45 [fp8_utils.py:777] Using default W8A8 Block FP8 kernel config. Performance might be sub-optimal! Config file not found at /home/jovyan/work/skku_venv/lib/python3.12/site-packages/vllm/model_executor/layers/quantization/utils/configs/N=5120,K=2048,device_name=NVIDIA_RTX_6000_Ada_Generation,dtype=fp8_w8a8,block_shape=[128,128].json\n",
      "\u001b[1;36m(EngineCore_DP0 pid=28616)\u001b[0;0m WARNING 12-02 09:59:46 [fp8_utils.py:777] Using default W8A8 Block FP8 kernel config. Performance might be sub-optimal! Config file not found at /home/jovyan/work/skku_venv/lib/python3.12/site-packages/vllm/model_executor/layers/quantization/utils/configs/N=2048,K=4096,device_name=NVIDIA_RTX_6000_Ada_Generation,dtype=fp8_w8a8,block_shape=[128,128].json\n",
      "\u001b[1;36m(EngineCore_DP0 pid=28616)\u001b[0;0m WARNING 12-02 09:59:46 [fused_moe.py:886] Using default MoE config. Performance might be sub-optimal! Config file not found at ['/home/jovyan/work/skku_venv/lib/python3.12/site-packages/vllm/model_executor/layers/fused_moe/configs/E=128,N=768,device_name=NVIDIA_RTX_6000_Ada_Generation,dtype=fp8_w8a8,block_shape=[128,128].json']\n",
      "\u001b[1;36m(EngineCore_DP0 pid=28616)\u001b[0;0m INFO 12-02 09:59:47 [monitor.py:34] torch.compile takes 17.15 s in total\n",
      "\u001b[1;36m(EngineCore_DP0 pid=28616)\u001b[0;0m INFO 12-02 09:59:49 [gpu_worker.py:359] Available KV cache memory: 11.67 GiB\n",
      "\u001b[1;36m(EngineCore_DP0 pid=28616)\u001b[0;0m INFO 12-02 09:59:49 [kv_cache_utils.py:1229] GPU KV cache size: 127,488 tokens\n",
      "\u001b[1;36m(EngineCore_DP0 pid=28616)\u001b[0;0m INFO 12-02 09:59:49 [kv_cache_utils.py:1234] Maximum concurrency for 40,960 tokens per request: 3.11x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 51/51 [00:08<00:00,  6.00it/s]\n",
      "Capturing CUDA graphs (decode, FULL): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 35/35 [00:04<00:00,  7.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(EngineCore_DP0 pid=28616)\u001b[0;0m INFO 12-02 10:00:03 [gpu_model_runner.py:4244] Graph capturing finished in 14 secs, took 0.92 GiB\n",
      "\u001b[1;36m(EngineCore_DP0 pid=28616)\u001b[0;0m INFO 12-02 10:00:03 [core.py:250] init engine (profile, create kv cache, warmup model) took 38.25 seconds\n",
      "INFO 12-02 10:00:04 [llm.py:352] Supported tasks: ['generate']\n"
     ]
    }
   ],
   "source": [
    "# 2. vLLM ì¶”ë¡  ì—”ì§„ ì´ˆê¸°í™” (Inference Engine Initialization)\n",
    "# LLM í´ë˜ìŠ¤: ê³ ì„±ëŠ¥ ì¶”ë¡ ì„ ìœ„í•œ vLLM ì—”ì§„ ì¸ìŠ¤í„´ìŠ¤ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.\n",
    "llm = LLM(\n",
    "    model=model_name,\n",
    "    \n",
    "    # [ë°ì´í„° ì •ë°€ë„ ì„¤ì •]\n",
    "    # \"auto\": ëª¨ë¸ ì„¤ì • íŒŒì¼(config.json)ì„ ê°ì§€í•˜ì—¬ ìµœì ì˜ ì •ë°€ë„ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤.\n",
    "    # ë³¸ ëª¨ë¸ì€ FP8 í˜•ì‹ì´ë¯€ë¡œ, ì´ë¥¼ ì¸ì‹í•˜ì—¬ RTX 6000 Ada ë“±ì˜ GPUì—ì„œ Tensor Core ê°€ì†ì„ í™œì„±í™”í•©ë‹ˆë‹¤.\n",
    "    dtype=\"auto\",\n",
    "\n",
    "    # [ë¶„ì‚° ì²˜ë¦¬ ì„¤ì •]\n",
    "    # tensor_parallel_size: ëª¨ë¸ì˜ ê°€ì¤‘ì¹˜ í–‰ë ¬(Weight Matrix)ì„ ëª‡ ê°œì˜ GPUì— ë¶„í• í•˜ì—¬ ì ì¬í• ì§€ ê²°ì •í•©ë‹ˆë‹¤.\n",
    "    # 1ë¡œ ì„¤ì • ì‹œ ë‹¨ì¼ GPUì—ì„œ ëª¨ë“  ì—°ì‚°ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.\n",
    "    tensor_parallel_size=1,\n",
    "\n",
    "    # [ë©”ëª¨ë¦¬ ê´€ë¦¬ ì„¤ì •]\n",
    "    # gpu_memory_utilization: GPU VRAMì˜ í• ë‹¹ ë¹„ìœ¨(0.0 ~ 1.0)ì„ ì„¤ì •í•©ë‹ˆë‹¤.\n",
    "    # 0.95: ê°€ì¤‘ì¹˜ ë¡œë“œ í›„ ë‚¨ì€ VRAMì˜ 95%ë¥¼ KV Cache ë¸”ë¡ìœ¼ë¡œ ì‚¬ì „ í• ë‹¹í•©ë‹ˆë‹¤.\n",
    "    # ì´ëŠ” ê¸´ ì»¨í…ìŠ¤íŠ¸ ì²˜ë¦¬ë¥¼ ìœ„í•œ ë©”ëª¨ë¦¬ ê³µê°„ì„ í™•ë³´í•˜ì—¬ Throughputì„ ìµœëŒ€í™”í•˜ê¸° ìœ„í•¨ì…ë‹ˆë‹¤.\n",
    "    gpu_memory_utilization=0.95 # VRAM 48GBë¥¼ ê½‰ ì±„ì›Œ ì“°ë„ë¡ ì„¤ì •\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1f457f9b-ca06-4b69-b1ce-5c0730db6b62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. ìƒ˜í”Œë§ ì„¤ì •\n",
    "# SamplingParams: ë‹¤ìŒ í† í°ì„ Samplingí•  ë•Œ ì‚¬ìš©í•  í™•ë¥ ì  ì•Œê³ ë¦¬ì¦˜ì˜ í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¥¼ ì •ì˜í•©ë‹ˆë‹¤.\n",
    "sampling_params = SamplingParams(\n",
    "    # [í™•ë¥  ë¶„í¬ ì¡°ì ˆ]\n",
    "    # temperature: Softmax ì¶œë ¥ í™•ë¥  ë¶„í¬ì˜ Flatness ì •ë„ë¥¼ ì¡°ì ˆí•©ë‹ˆë‹¤.\n",
    "    # 0.7: ë¶„í¬ë¥¼ ì ë‹¹íˆ í‰íƒ„í•˜ê²Œ í•˜ì—¬, ê°€ì¥ ë†’ì€ í™•ë¥ ì˜ í† í° ì™¸ì—ë„ ì°¨ìˆœìœ„ í† í°ì´ ì„ íƒë  ê°€ëŠ¥ì„±ì„ ë¶€ì—¬í•©ë‹ˆë‹¤ (ìƒì„± ë‹¤ì–‘ì„± í™•ë³´).\n",
    "    temperature=0.7, \n",
    "\n",
    "    # [í›„ë³´êµ° ì œí•œ]\n",
    "    # top_p: Cumulative Probabilityê°€ 80%ì— ë„ë‹¬í•˜ëŠ” ìƒìœ„ í† í° ì§‘í•© ë‚´ì—ì„œë§Œ ìƒ˜í”Œë§ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.\n",
    "    # í™•ë¥ ì´ ë§¤ìš° ë‚®ì€ í† í°ì˜ ìƒì„±ì„ ë°°ì œí•˜ì—¬ í…ìŠ¤íŠ¸ì˜ í’ˆì§ˆì„ ìœ ì§€í•©ë‹ˆë‹¤.\n",
    "    top_p=0.8, \n",
    "\n",
    "    # [ë°˜ë³µ ì–µì œ]\n",
    "    # repetition_penalty: ì´ë¯¸ ìƒì„±ëœ í† í°ì˜ ë¡œì§“(Logit) ê°’ì„ ì¸ìœ„ì ìœ¼ë¡œ ë‚®ì¶”ì–´ ì¬ì„ íƒ í™•ë¥ ì„ ê°ì†Œì‹œí‚µë‹ˆë‹¤.\n",
    "    # 1.05: 5%ì˜ í˜ë„í‹°ë¥¼ ë¶€ì—¬í•˜ì—¬, ë™ì¼í•œ ë¬¸ì¥ì´ ë¬´í•œ ë°˜ë³µ(Infinite Loop)ë˜ëŠ” í˜„ìƒì„ ë°©ì§€í•©ë‹ˆë‹¤.\n",
    "    repetition_penalty=1.05,\n",
    "\n",
    "    # [ìƒì„± ê¸¸ì´ ì œí•œ]\n",
    "    # max_tokens: ëª¨ë¸ì´ ìƒì„±í•  ìˆ˜ ìˆëŠ” ìµœëŒ€ ì‹ ê·œ í† í° ìˆ˜(Budget)ë¥¼ ì œí•œí•©ë‹ˆë‹¤.\n",
    "    max_tokens=4096 # ìµœëŒ€ 32768\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f3b75548-d7f8-4598-8e47-5bdac97a7f05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. í”„ë¡¬í”„íŠ¸ ì—”ì§€ë‹ˆì–´ë§ (Prompt Construction)\n",
    "prompt = \"ì„±ê· ê´€ëŒ€í•™êµê°€ ìµœê³ ì˜ ëŒ€í•™êµì¸ ì´ìœ  í•œ ê°€ì§€ ë§í•´ì¤˜.\"\n",
    "\n",
    "# [ëŒ€í™”í˜• êµ¬ì¡° ì •ì˜]\n",
    "# ë¦¬ìŠ¤íŠ¸ ë‚´ ë”•ì…”ë„ˆë¦¬ êµ¬ì¡°ë¥¼ ì‚¬ìš©í•˜ì—¬ ì‹œìŠ¤í…œê³¼ ì‚¬ìš©ìì˜ ì—­í• ì„ ëª…ì‹œí•©ë‹ˆë‹¤.\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}, # System instructionì€ ì—¬ê¸°ì— ì ìœ¼ë©´ ë©ë‹ˆë‹¤.\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "]\n",
    "\n",
    "# [ì±„íŒ… í…œí”Œë¦¿ ì ìš©]\n",
    "# apply_chat_template: êµ¬ì¡°í™”ëœ ë©”ì‹œì§€ ë¦¬ìŠ¤íŠ¸ë¥¼ ëª¨ë¸ í•™ìŠµ ì‹œ ì‚¬ìš©ëœ íŠ¹ìˆ˜ í† í°(Special Tokens)ì´ í¬í•¨ëœ ë‹¨ì¼ ë¬¸ìì—´ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.\n",
    "# ì˜ˆ: \"<|im_start|>system\\nYou are...<|im_end|>\\n<|im_start|>user\\nTell me...<|im_end|>\\n<|im_start|>assistant\"\n",
    "# tokenize=False: í† í° IDê°€ ì•„ë‹Œ ê°€ê³µëœ í…ìŠ¤íŠ¸ ë¬¸ìì—´(Raw String)ì„ ë°˜í™˜ë°›ìŠµë‹ˆë‹¤ (vLLM ì…ë ¥ìš©).\n",
    "# add_generation_prompt=True: ëª¨ë¸ì´ ë‹µë³€ì„ ì‹œì‘í•  ìˆ˜ ìˆë„ë¡ ì–´ì‹œìŠ¤í„´íŠ¸ì˜ ì‹œì‘ í† í°ì„ í”„ë¡¬í”„íŠ¸ ëì— ìë™ìœ¼ë¡œ ì¶”ê°€í•©ë‹ˆë‹¤.\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "795fc95a-4807-458d-ae26-d14338fba576",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c335b832d0e46ee86dbb7d91a1b61c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b534a9045aba48ec821389efd0a45934",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "outputs = llm.generate([text], sampling_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d9009118-0547-41a2-8ba8-a48afd36e33c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì…ë ¥: <|im_start|>system\n",
      "You are a helpful assistant.<|im_end|>\n",
      "<|im_start|>user\n",
      "ì„±ê· ê´€ëŒ€í•™êµê°€ ìµœê³ ì˜ ëŒ€í•™êµì¸ ì´ìœ  í•œ ê°€ì§€ ë§í•´ì¤˜.<|im_end|>\n",
      "<|im_start|>assistant\n",
      "\n",
      "\n",
      "ì‚¬ê³ : Okay, the user is asking why Sungkyunkwan University is the best, and they want just one reason. Let me think about what makes SKKU stand out.\n",
      "\n",
      "First, I know that SKKU is one of the oldest universities in Korea, founded in 1398. That's a long history, which gives it a lot of prestige. But is that the main reason people consider it the best? Maybe not just the age, but how it's maintained its reputation over time.\n",
      "\n",
      "Another point is its academic excellence. SKKU has strong programs in various fields, especially in engineering, business, and humanities. They might have good research facilities and faculty. Also, their international partnerships and exchange programs could be a factor. But again, is that the top reason?\n",
      "\n",
      "Then there's the campus and location. The main campus is in Seoul, which is a big city with lots of opportunities. The architecture is also notable, like the traditional buildings. But does that make it the best? Maybe for some students, but not sure if it's the primary reason.\n",
      "\n",
      "I should check if there are any unique programs or initiatives. For example, SKKU might have a focus on innovation or specific research centers. Or maybe their alumni network is strong, leading to good career prospects. However, the user asked for one reason, so I need to pick the most significant one.\n",
      "\n",
      "Wait, the user might be looking for something specific. In Korean context, the historical significance and cultural heritage are important. SKKU was originally a Confucian academy, which adds to its uniqueness. This blend of tradition and modern education could be a key point. It's not just an old university but also maintains a balance between traditional values and contemporary education.\n",
      "\n",
      "Alternatively, maybe the quality of education and research output. If SKKU is ranked high in certain areas or has notable achievements, that could be the reason. But without specific data, it's safer to go with the historical and cultural aspect since it's a defining feature.\n",
      "\n",
      "So, the answer would highlight its status as the oldest university in Korea, combining centuries-old traditions with modern academic excellence, making it a prestigious institution.\n",
      "\n",
      "ë‹µë³€: ì„±ê· ê´€ëŒ€í•™êµëŠ” í•œêµ­ì—ì„œ ê°€ì¥ ì˜¤ë˜ëœ ëŒ€í•™ ì¤‘ í•˜ë‚˜ë¡œ, 1398ë…„ì— ì°½ë¦½ë˜ì–´ ì „í†µê³¼ í˜„ëŒ€ êµìœ¡ì˜ ì¡°í™”ë¥¼ ì´ë£¬ ì ì—ì„œ ìµœê³ ì˜ ëŒ€í•™êµë¡œ ì¸ì •ë°›ìŠµë‹ˆë‹¤. ì´ í•™êµëŠ” ì¡°ì„  ì‹œëŒ€ì˜ ìœ êµ êµìœ¡ ê¸°ë°˜ì„ ë°”íƒ•ìœ¼ë¡œ í•˜ë©°, í˜„ì¬ëŠ” ê³¼í•™, ê³µí•™, ê²½ì˜, ì¸ë¬¸í•™ ë“± ë‹¤ì–‘í•œ ë¶„ì•¼ì—ì„œ ë†’ì€ í•™ìˆ ì  ëª…ì„±ì„ ìœ ì§€í•˜ê³  ìˆìŠµë‹ˆë‹¤. ì—­ì‚¬ì  ìœ ì‚°ê³¼ í˜„ëŒ€ì  ì—°êµ¬ ì—­ëŸ‰ì„ ë™ì‹œì— ê°–ì¶˜ ì ì—ì„œ ê·¸ ê°€ì¹˜ê°€ ë‘ë“œëŸ¬ì§‘ë‹ˆë‹¤.\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "for output in outputs:\n",
    "    prompt = output.prompt\n",
    "    generated_text = output.outputs[0].text.strip()\n",
    "\n",
    "    # ë³€ìˆ˜ ì´ˆê¸°í™”\n",
    "    thinking_content = None\n",
    "    final_response = generated_text\n",
    "\n",
    "    # Case 1: </think> (ë‹«ëŠ” íƒœê·¸)ê°€ ëª…í™•íˆ ìˆëŠ” ê²½ìš° -> ê°€ì¥ ì¼ë°˜ì \n",
    "    if \"</think>\" in generated_text:\n",
    "        parts = generated_text.split(\"</think>\")\n",
    "        \n",
    "        # ì•ë¶€ë¶„: <think> íƒœê·¸ ì‚­ì œ í›„ 'ì‚¬ê³ ' ë‚´ìš©ìœ¼ë¡œ ì €ì¥\n",
    "        thinking_content = parts[0].replace(\"<think>\", \"\").strip()\n",
    "        \n",
    "        # ë’·ë¶€ë¶„: ì‹¤ì œ ë‹µë³€\n",
    "        if len(parts) > 1:\n",
    "            final_response = parts[1].strip()\n",
    "        else:\n",
    "            final_response = \"\" # ìƒê°ë§Œ í•˜ê³  ë‹µë³€ì€ ì•„ì§ ì•ˆ ë‚˜ì˜¨ ê²½ìš°\n",
    "\n",
    "    print(f\"ì…ë ¥: {prompt}\")\n",
    "    print()\n",
    "\n",
    "    if thinking_content:\n",
    "        print(f\"ì‚¬ê³ : {thinking_content}\")\n",
    "        print()\n",
    "\n",
    "    if final_response:\n",
    "        print(f\"ë‹µë³€: {final_response}\")\n",
    "    \n",
    "    print(\"-\" * 50) # êµ¬ë¶„ì„ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ab8632c9-e2fe-4aa0-8548-ab6e6c568aaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë©€í‹°í„´ ëŒ€í™” í•´ë³´ê¸°\n",
    "from vllm import LLM, SamplingParams\n",
    "from transformers import AutoTokenizer\n",
    "import re\n",
    "\n",
    "class QwenVLLMChatbot:\n",
    "    def __init__(self, model_name=\"Qwen/Qwen3-30B-A3B-FP8\", llm=None, tokenizer=None):\n",
    "        \"\"\"\n",
    "        ì±—ë´‡ ì¸ìŠ¤í„´ìŠ¤ ì´ˆê¸°í™” (Initialization)\n",
    "        \n",
    "        Args:\n",
    "            model_name (str): ë¡œë“œí•  ëª¨ë¸ì˜ ê²½ë¡œ ë˜ëŠ” Hugging Face ID.\n",
    "            llm (vllm.LLM, optional): ì´ë¯¸ ì´ˆê¸°í™”ëœ vLLM ì—”ì§„ ì¸ìŠ¤í„´ìŠ¤. ì œê³µë  ê²½ìš° ìƒˆë¡œ ë¡œë“œí•˜ì§€ ì•ŠìŒ.\n",
    "            tokenizer (AutoTokenizer, optional): ì´ë¯¸ ì´ˆê¸°í™”ëœ í† í¬ë‚˜ì´ì € ì¸ìŠ¤í„´ìŠ¤.\n",
    "        \"\"\"\n",
    "        print(f\"Initializing Chatbot with model: {model_name}...\")\n",
    "        \n",
    "        # 1. vLLM ì—”ì§„ í• ë‹¹ (Dependency Injection)\n",
    "        # ì™¸ë¶€ì—ì„œ ì „ë‹¬ëœ llm ê°ì²´ê°€ ìˆìœ¼ë©´ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•˜ê³ , ì—†ìœ¼ë©´ ìƒˆë¡œ ìƒì„±\n",
    "        # ì´ë¥¼ í†µí•´ ë°˜ë³µì ì¸ VRAM ì¬í• ë‹¹ ë° ë¡œë”© ì‹œê°„ì„ ë°©ì§€\n",
    "        if llm:\n",
    "            print(\">> Existing LLM instance detected. Using the provided engine.\")\n",
    "            self.llm = llm\n",
    "        else:\n",
    "            print(\">> No LLM instance provided. Loading new engine...\")\n",
    "            self.llm = LLM(\n",
    "                model=model_name,\n",
    "                tensor_parallel_size=1,     # ë‹¨ì¼ GPU ì‚¬ìš©\n",
    "                gpu_memory_utilization=0.95, # VRAM ì ìœ ìœ¨ ìµœëŒ€í™”\n",
    "                dtype=\"auto\" # FP8 ëª¨ë¸ì˜ ê²½ìš° auto ì„¤ì • ì‹œ config.jsonì— ë§ì¶° ìë™ ìµœì í™”ë¨\n",
    "            )\n",
    "\n",
    "        # 2. í† í¬ë‚˜ì´ì € í• ë‹¹\n",
    "        if tokenizer:\n",
    "            print(\">> Existing Tokenizer instance detected. Using the provided tokenizer.\")\n",
    "            self.tokenizer = tokenizer\n",
    "        else:\n",
    "            print(\">> Loading new tokenizer...\")\n",
    "            self.tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "\n",
    "        # 3. ìƒíƒœ ë³€ìˆ˜ ì´ˆê¸°í™”\n",
    "        self.history = []  # ëŒ€í™” ë¬¸ë§¥(Context) ì €ì¥ì†Œ\n",
    "\n",
    "    def generate_response(self, user_input):\n",
    "        \"\"\"\n",
    "        ì‚¬ìš©ì ì…ë ¥ì„ ë°›ì•„ ëª¨ë¸ì˜ ì¶”ë¡  ê²°ê³¼ë¥¼ ë°˜í™˜í•˜ê³  íˆìŠ¤í† ë¦¬ë¥¼ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤.\n",
    "        \"\"\"\n",
    "        # 1. í”„ë¡¬í”„íŠ¸ êµ¬ì„±\n",
    "        # í˜„ì¬ê¹Œì§€ì˜ íˆìŠ¤í† ë¦¬ì— ì´ë²ˆ í„´ì˜ ì‚¬ìš©ì ì…ë ¥ì„ ì„ì‹œë¡œ ì¶”ê°€í•˜ì—¬ í…œí”Œë¦¿ì„ ì ìš©\n",
    "        current_messages = self.history + [{\"role\": \"user\", \"content\": user_input}]\n",
    "        \n",
    "        prompt_str = self.tokenizer.apply_chat_template(\n",
    "            current_messages,\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=True\n",
    "        )\n",
    "\n",
    "        # 2. ìƒ˜í”Œë§ íŒŒë¼ë¯¸í„° ì„¤ì •\n",
    "        sampling_params = SamplingParams(\n",
    "            temperature=0.7,\n",
    "            top_p=0.8,\n",
    "            max_tokens=4096,\n",
    "            repetition_penalty=1.05,\n",
    "        )\n",
    "\n",
    "        # 3. ì¶”ë¡ \n",
    "        outputs = self.llm.generate([prompt_str], sampling_params)\n",
    "        raw_output = outputs[0].outputs[0].text.strip()\n",
    "\n",
    "        # 4. ê²°ê³¼ íŒŒì‹±\n",
    "        # <think> íƒœê·¸ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ë‚´ë¶€ ì‚¬ê³  ê³¼ì •ê³¼ ìµœì¢… ë‹µë³€ì„ ë¶„ë¦¬\n",
    "        thinking_content = \"\"\n",
    "        final_response = raw_output\n",
    "\n",
    "        if \"</think>\" in raw_output:\n",
    "            parts = raw_output.split(\"</think>\")\n",
    "            thinking_content = parts[0].replace(\"<think>\", \"\").strip()\n",
    "            if len(parts) > 1:\n",
    "                final_response = parts[1].strip()\n",
    "            else:\n",
    "                final_response = \"\" # ì‚¬ê³  ê³¼ì •ë§Œ ì¶œë ¥ëœ ê²½ìš° ì˜ˆì™¸ ì²˜ë¦¬\n",
    "\n",
    "        # 5. íˆìŠ¤í† ë¦¬ ì—…ë°ì´íŠ¸\n",
    "        # Context Window ê´€ë¦¬ë¥¼ ìœ„í•´ ìµœì¢… ë‹µë³€ë§Œ ê¸°ë¡\n",
    "        self.history.append({\"role\": \"user\", \"content\": user_input})\n",
    "        self.history.append({\"role\": \"assistant\", \"content\": final_response})\n",
    "\n",
    "        return {\n",
    "            \"thought\": thinking_content,\n",
    "            \"response\": final_response,\n",
    "            \"raw\": raw_output\n",
    "        }\n",
    "\n",
    "    def clear_history(self):\n",
    "        \"\"\"ëŒ€í™” ë‚´ì—­ì„ ì´ˆê¸°í™”í•©ë‹ˆë‹¤.\"\"\"\n",
    "        self.history = []\n",
    "        print(\">> Chat history cleared.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "78972cac-0449-450e-be56-381cd525e672",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Chatbot with model: Qwen/Qwen3-30B-A3B-FP8...\n",
      ">> Existing LLM instance detected. Using the provided engine.\n",
      ">> Existing Tokenizer instance detected. Using the provided tokenizer.\n",
      "\n",
      "[ì…ë ¥]: ë”¸ê¸°(Strawberry) ë‹¨ì–´ì— rì´ ëª‡ ê°œ ë“¤ì–´ìˆì–´?\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb6ecbe6ca1644f59875f25fe4e0be83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e944bbec3eec43a996caefa997af0d22",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Turn 1 Result]\n",
      "[ì‚¬ê³ ]: Okay, let's see. The user is asking how many times the letter 'r' appears in the word \"Strawberry.\" ... (ìƒëµ)\n",
      "[ë‹µë³€]: ë”¸ê¸°(\"Strawberry\") ë‹¨ì–´ì— í¬í•¨ëœ **r**ì˜ ê°œìˆ˜ë¥¼ í™•ì¸í•´ë³´ë©´ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:\n",
      "\n",
      "1. **S**  \n",
      "2. **T**  \n",
      "3. **R** â†’ 1ê°œ  \n",
      "4. **A**  \n",
      "5. **W**  \n",
      "6. **B**  \n",
      "7. **E**  \n",
      "8. **R** â†’ 2ê°œ  \n",
      "9. **R** â†’ 3ê°œ  \n",
      "10. **Y**  \n",
      "\n",
      "ë”°ë¼ì„œ **rì€ ì´ 3ê°œ** ë“¤ì–´ ìˆìŠµë‹ˆë‹¤.  \n",
      "\n",
      "**ì •ë‹µ:** 3ê°œ âœ…\n",
      "\n",
      "[ì…ë ¥]: ê·¸ëŸ¼ 'Raspberry'ëŠ”?\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da9581816d4b4805b200340ed142caba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc7097fc5fe14d3b82d7e65423a11182",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Turn 2 Result]\n",
      "[ì‚¬ê³ ]: Okay, the user asked about the number of 'r's in \"Raspberry\" after previously asking about \"Strawber... (ìƒëµ)\n",
      "[ë‹µë³€]: 'Raspberry' ë‹¨ì–´ì— í¬í•¨ëœ **r**ì˜ ê°œìˆ˜ë¥¼ í™•ì¸í•´ë³´ë©´ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:\n",
      "\n",
      "1. **R** â†’ 1ê°œ  \n",
      "2. **A**  \n",
      "3. **S**  \n",
      "4. **P**  \n",
      "5. **B**  \n",
      "6. **E**  \n",
      "7. **R** â†’ 2ê°œ  \n",
      "8. **R** â†’ 3ê°œ  \n",
      "9. **Y**  \n",
      "\n",
      "ë”°ë¼ì„œ **rì€ ì´ 3ê°œ** ë“¤ì–´ ìˆìŠµë‹ˆë‹¤.  \n",
      "\n",
      "**ì •ë‹µ:** 3ê°œ âœ…  \n",
      "\n",
      "(ì°¸ê³ : \"Strawberry\"ì™€ ë§ˆì°¬ê°€ì§€ë¡œ \"Raspberry\"ë„ 3ê°œì˜ 'r'ì„ ê°€ì§€ê³  ìˆì–´ìš”!) ğŸ“\n"
     ]
    }
   ],
   "source": [
    "chatbot = QwenVLLMChatbot(llm=llm, tokenizer=tokenizer)\n",
    "\n",
    "def print_result(turn, result):\n",
    "    print(f\"\\n[Turn {turn} Result]\")\n",
    "    if result[\"thought\"]:\n",
    "        print(f\"[ì‚¬ê³ ]: {result['thought'][:100]}... (ìƒëµ)\")\n",
    "    print(f\"[ë‹µë³€]: {result['response']}\")\n",
    "\n",
    "# Turn 1\n",
    "q1 = \"ë”¸ê¸°(Strawberry) ë‹¨ì–´ì— rì´ ëª‡ ê°œ ë“¤ì–´ìˆì–´?\"\n",
    "print(f\"\\n[ì…ë ¥]: {q1}\")\n",
    "res1 = chatbot.generate_response(q1)\n",
    "print_result(1, res1)\n",
    "\n",
    "# Turn 2 (ë¬¸ë§¥ ìœ ì§€ í™•ì¸)\n",
    "q2 = \"ê·¸ëŸ¼ 'Raspberry'ëŠ”?\"\n",
    "print(f\"\\n[ì…ë ¥]: {q2}\")\n",
    "res2 = chatbot.generate_response(q2)\n",
    "print_result(2, res2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7446b5c2-0e05-4b67-abcc-0dc14ea92d08",
   "metadata": {},
   "outputs": [],
   "source": [
    "chatbot.clear_history()\n",
    "\n",
    "idx = 1\n",
    "while True:\n",
    "    query = input()\n",
    "    response = chatbot.generate_response(query)\n",
    "    print_result(idx, response)\n",
    "    idx += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16d68d55-d5d8-4a45-856e-e8fc85a014fc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "skku_venv",
   "language": "python",
   "name": "skku_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
