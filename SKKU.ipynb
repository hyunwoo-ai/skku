{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f1f37f7-d4f9-4d75-a10f-36a68ee6dc51",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from vllm import LLM, SamplingParams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a29360b-f9ad-48ed-a517-53b50f326dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 사용할 모델 식별자 정의 (Hugging Face Model ID)\n",
    "# Qwen3-30B-A3B-FP8: MoE 아키텍처 기반이며, FP8(8-bit Floating Point)로 양자화된 모델\n",
    "model_name = \"Qwen/Qwen3-30B-A3B-FP8\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ceaf99d-27bf-4562-99cd-acdc34417a31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. 토크나이저 초기화 (Tokenizer Initialization)\n",
    "# AutoTokenizer: 입력 텍스트(String)를 모델이 처리 가능한 수치 데이터인 'Token ID(Integer)'로 변환하는 전처리 모듈입니다.\n",
    "# - 역할: 모델의 고유 어휘 집합(Vocabulary)에 매핑된 인덱스를 반환하며, 모델은 이 인덱스를 통해 임베딩 레이어(Embedding Layer)의 벡터 값을 참조합니다.\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dab118ab-546c-4202-8fe9-255f37063994",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. vLLM 추론 엔진 초기화 (Inference Engine Initialization)\n",
    "# LLM 클래스: 고성능 추론을 위한 vLLM 엔진 인스턴스를 생성합니다.\n",
    "llm = LLM(\n",
    "    model=model_name,\n",
    "    \n",
    "    # [데이터 정밀도 설정]\n",
    "    # \"auto\": 모델 설정 파일(config.json)을 감지하여 최적의 정밀도를 로드합니다.\n",
    "    # 본 모델은 FP8 형식이므로, 이를 인식하여 RTX 6000 Ada 등의 GPU에서 Tensor Core 가속을 활성화합니다.\n",
    "    dtype=\"auto\",\n",
    "\n",
    "    # [분산 처리 설정]\n",
    "    # tensor_parallel_size: 모델의 가중치 행렬(Weight Matrix)을 몇 개의 GPU에 분할하여 적재할지 결정합니다.\n",
    "    # 1로 설정 시 단일 GPU에서 모든 연산을 수행합니다.\n",
    "    tensor_parallel_size=1,\n",
    "\n",
    "    # [메모리 관리 설정]\n",
    "    # gpu_memory_utilization: GPU VRAM의 할당 비율(0.0 ~ 1.0)을 설정합니다.\n",
    "    # 0.95: 가중치 로드 후 남은 VRAM의 95%를 KV Cache 블록으로 사전 할당합니다.\n",
    "    # 이는 긴 컨텍스트 처리를 위한 메모리 공간을 확보하여 Throughput을 최대화하기 위함입니다.\n",
    "    gpu_memory_utilization=0.95 # VRAM 48GB를 꽉 채워 쓰도록 설정\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f457f9b-ca06-4b69-b1ce-5c0730db6b62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. 샘플링 설정\n",
    "# SamplingParams: 다음 토큰을 Sampling할 때 사용할 확률적 알고리즘의 하이퍼파라미터를 정의합니다.\n",
    "sampling_params = SamplingParams(\n",
    "    # [확률 분포 조절]\n",
    "    # temperature: Softmax 출력 확률 분포의 Flatness 정도를 조절합니다.\n",
    "    # 0.7: 분포를 적당히 평탄하게 하여, 가장 높은 확률의 토큰 외에도 차순위 토큰이 선택될 가능성을 부여합니다 (생성 다양성 확보).\n",
    "    temperature=0.7, \n",
    "\n",
    "    # [후보군 제한]\n",
    "    # top_p: Cumulative Probability가 80%에 도달하는 상위 토큰 집합 내에서만 샘플링을 수행합니다.\n",
    "    # 확률이 매우 낮은 토큰의 생성을 배제하여 텍스트의 품질을 유지합니다.\n",
    "    top_p=0.8, \n",
    "\n",
    "    # [반복 억제]\n",
    "    # repetition_penalty: 이미 생성된 토큰의 로짓(Logit) 값을 인위적으로 낮추어 재선택 확률을 감소시킵니다.\n",
    "    # 1.05: 5%의 페널티를 부여하여, 동일한 문장이 무한 반복(Infinite Loop)되는 현상을 방지합니다.\n",
    "    repetition_penalty=1.05,\n",
    "\n",
    "    # [생성 길이 제한]\n",
    "    # max_tokens: 모델이 생성할 수 있는 최대 신규 토큰 수(Budget)를 제한합니다.\n",
    "    max_tokens=4096 # 최대 32768\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3b75548-d7f8-4598-8e47-5bdac97a7f05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. 프롬프트 엔지니어링 (Prompt Construction)\n",
    "prompt = \"성균관대학교가 최고의 대학교인 이유 한 가지 말해줘.\"\n",
    "\n",
    "# [대화형 구조 정의]\n",
    "# 리스트 내 딕셔너리 구조를 사용하여 시스템과 사용자의 역할을 명시합니다.\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}, # System instruction은 여기에 적으면 됩니다.\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "]\n",
    "\n",
    "# [채팅 템플릿 적용]\n",
    "# apply_chat_template: 구조화된 메시지 리스트를 모델 학습 시 사용된 특수 토큰(Special Tokens)이 포함된 단일 문자열로 변환합니다.\n",
    "# 예: \"<|im_start|>system\\nYou are...<|im_end|>\\n<|im_start|>user\\nTell me...<|im_end|>\\n<|im_start|>assistant\"\n",
    "# tokenize=False: 토큰 ID가 아닌 가공된 텍스트 문자열(Raw String)을 반환받습니다 (vLLM 입력용).\n",
    "# add_generation_prompt=True: 모델이 답변을 시작할 수 있도록 어시스턴트의 시작 토큰을 프롬프트 끝에 자동으로 추가합니다.\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "795fc95a-4807-458d-ae26-d14338fba576",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = llm.generate([text], sampling_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9009118-0547-41a2-8ba8-a48afd36e33c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "for output in outputs:\n",
    "    prompt = output.prompt\n",
    "    generated_text = output.outputs[0].text.strip()\n",
    "\n",
    "    # 변수 초기화\n",
    "    thinking_content = None\n",
    "    final_response = generated_text\n",
    "\n",
    "    # Case 1: </think> (닫는 태그)가 명확히 있는 경우 -> 가장 일반적\n",
    "    if \"</think>\" in generated_text:\n",
    "        parts = generated_text.split(\"</think>\")\n",
    "        \n",
    "        # 앞부분: <think> 태그 삭제 후 '사고' 내용으로 저장\n",
    "        thinking_content = parts[0].replace(\"<think>\", \"\").strip()\n",
    "        \n",
    "        # 뒷부분: 실제 답변\n",
    "        if len(parts) > 1:\n",
    "            final_response = parts[1].strip()\n",
    "        else:\n",
    "            final_response = \"\" # 생각만 하고 답변은 아직 안 나온 경우\n",
    "\n",
    "    print(f\"입력: {prompt}\")\n",
    "    print()\n",
    "\n",
    "    if thinking_content:\n",
    "        print(f\"사고: {thinking_content}\")\n",
    "        print()\n",
    "\n",
    "    if final_response:\n",
    "        print(f\"답변: {final_response}\")\n",
    "    \n",
    "    print(\"-\" * 50) # 구분선"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab8632c9-e2fe-4aa0-8548-ab6e6c568aaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 멀티턴 대화 해보기\n",
    "from vllm import LLM, SamplingParams\n",
    "from transformers import AutoTokenizer\n",
    "import re\n",
    "\n",
    "class QwenVLLMChatbot:\n",
    "    def __init__(self, model_name=\"Qwen/Qwen3-30B-A3B-FP8\", llm=None, tokenizer=None):\n",
    "        \"\"\"\n",
    "        챗봇 인스턴스 초기화 (Initialization)\n",
    "        \n",
    "        Args:\n",
    "            model_name (str): 로드할 모델의 경로 또는 Hugging Face ID.\n",
    "            llm (vllm.LLM, optional): 이미 초기화된 vLLM 엔진 인스턴스. 제공될 경우 새로 로드하지 않음.\n",
    "            tokenizer (AutoTokenizer, optional): 이미 초기화된 토크나이저 인스턴스.\n",
    "        \"\"\"\n",
    "        print(f\"Initializing Chatbot with model: {model_name}...\")\n",
    "        \n",
    "        # 1. vLLM 엔진 할당 (Dependency Injection)\n",
    "        # 외부에서 전달된 llm 객체가 있으면 그대로 사용하고, 없으면 새로 생성\n",
    "        # 이를 통해 반복적인 VRAM 재할당 및 로딩 시간을 방지\n",
    "        if llm:\n",
    "            print(\">> Existing LLM instance detected. Using the provided engine.\")\n",
    "            self.llm = llm\n",
    "        else:\n",
    "            print(\">> No LLM instance provided. Loading new engine...\")\n",
    "            self.llm = LLM(\n",
    "                model=model_name,\n",
    "                tensor_parallel_size=1,     # 단일 GPU 사용\n",
    "                gpu_memory_utilization=0.95, # VRAM 점유율 최대화\n",
    "                dtype=\"auto\" # FP8 모델의 경우 auto 설정 시 config.json에 맞춰 자동 최적화됨\n",
    "            )\n",
    "\n",
    "        # 2. 토크나이저 할당\n",
    "        if tokenizer:\n",
    "            print(\">> Existing Tokenizer instance detected. Using the provided tokenizer.\")\n",
    "            self.tokenizer = tokenizer\n",
    "        else:\n",
    "            print(\">> Loading new tokenizer...\")\n",
    "            self.tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "\n",
    "        # 3. 상태 변수 초기화\n",
    "        self.history = []  # 대화 문맥(Context) 저장소\n",
    "\n",
    "    def generate_response(self, user_input):\n",
    "        \"\"\"\n",
    "        사용자 입력을 받아 모델의 추론 결과를 반환하고 히스토리를 업데이트합니다.\n",
    "        \"\"\"\n",
    "        # 1. 프롬프트 구성\n",
    "        # 현재까지의 히스토리에 이번 턴의 사용자 입력을 임시로 추가하여 템플릿을 적용\n",
    "        current_messages = self.history + [{\"role\": \"user\", \"content\": user_input}]\n",
    "        \n",
    "        prompt_str = self.tokenizer.apply_chat_template(\n",
    "            current_messages,\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=True\n",
    "        )\n",
    "\n",
    "        # 2. 샘플링 파라미터 설정\n",
    "        sampling_params = SamplingParams(\n",
    "            temperature=0.7,\n",
    "            top_p=0.8,\n",
    "            max_tokens=4096,\n",
    "            repetition_penalty=1.05,\n",
    "        )\n",
    "\n",
    "        # 3. 추론\n",
    "        outputs = self.llm.generate([prompt_str], sampling_params)\n",
    "        raw_output = outputs[0].outputs[0].text.strip()\n",
    "\n",
    "        # 4. 결과 파싱\n",
    "        # <think> 태그를 기준으로 내부 사고 과정과 최종 답변을 분리\n",
    "        thinking_content = \"\"\n",
    "        final_response = raw_output\n",
    "\n",
    "        if \"</think>\" in raw_output:\n",
    "            parts = raw_output.split(\"</think>\")\n",
    "            thinking_content = parts[0].replace(\"<think>\", \"\").strip()\n",
    "            if len(parts) > 1:\n",
    "                final_response = parts[1].strip()\n",
    "            else:\n",
    "                final_response = \"\" # 사고 과정만 출력된 경우 예외 처리\n",
    "\n",
    "        # 5. 히스토리 업데이트\n",
    "        # Context Window 관리를 위해 최종 답변만 기록\n",
    "        self.history.append({\"role\": \"user\", \"content\": user_input})\n",
    "        self.history.append({\"role\": \"assistant\", \"content\": final_response})\n",
    "\n",
    "        return {\n",
    "            \"thought\": thinking_content,\n",
    "            \"response\": final_response,\n",
    "            \"raw\": raw_output\n",
    "        }\n",
    "\n",
    "    def clear_history(self):\n",
    "        \"\"\"대화 내역을 초기화합니다.\"\"\"\n",
    "        self.history = []\n",
    "        print(\">> Chat history cleared.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78972cac-0449-450e-be56-381cd525e672",
   "metadata": {},
   "outputs": [],
   "source": [
    "chatbot = QwenVLLMChatbot(llm=llm, tokenizer=tokenizer)\n",
    "\n",
    "def print_result(turn, result):\n",
    "    print(f\"\\n[Turn {turn} Result]\")\n",
    "    if result[\"thought\"]:\n",
    "        print(f\"[사고]: {result['thought'][:100]}... (생략)\")\n",
    "    print(f\"[답변]: {result['response']}\")\n",
    "\n",
    "# Turn 1\n",
    "q1 = \"딸기(Strawberry) 단어에 r이 몇 개 들어있어?\"\n",
    "print(f\"\\n[입력]: {q1}\")\n",
    "res1 = chatbot.generate_response(q1)\n",
    "print_result(1, res1)\n",
    "\n",
    "# Turn 2 (문맥 유지 확인)\n",
    "q2 = \"그럼 'Raspberry'는?\"\n",
    "print(f\"\\n[입력]: {q2}\")\n",
    "res2 = chatbot.generate_response(q2)\n",
    "print_result(2, res2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7446b5c2-0e05-4b67-abcc-0dc14ea92d08",
   "metadata": {},
   "outputs": [],
   "source": [
    "chatbot.clear_history()\n",
    "\n",
    "idx = 1\n",
    "while True:\n",
    "    query = input()\n",
    "    response = chatbot.generate_response(query)\n",
    "    print_result(idx, response)\n",
    "    idx += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16d68d55-d5d8-4a45-856e-e8fc85a014fc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "skku_venv",
   "language": "python",
   "name": "skku_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
