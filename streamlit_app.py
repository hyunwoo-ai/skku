import streamlit as st
import time
import random
import ast
from datetime import datetime

import instructions

# =======================================================
# 1. Backend Libraries & Classes
# =======================================================
try:
    from vllm import LLM, SamplingParams
    from transformers import AutoTokenizer
    from FlagEmbedding import BGEM3FlagModel
    import chromadb
except ImportError:
    # ë¡œì»¬ í™˜ê²½ì—ì„œ ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì—†ì„ ê²½ìš°ë¥¼ ëŒ€ë¹„í•œ ì•ˆë‚´
    st.error("í•„ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬(vllm, transformers, chromadb ë“±)ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    st.stop()

class QueryRewriter:
    def __init__(self, llm, tokenizer):
        self.llm = llm
        self.tokenizer = tokenizer

        # ì°½ì˜ì„±ë³´ë‹¤ëŠ” ì •í™•ì„±ì„ ìœ„í•´ temperatureë¥¼ ë‚®ì¶¤
        self.sampling_params = SamplingParams(temperature=0.2, max_tokens=32768, repetition_penalty=1.05,)
        
        # ê²€ìƒ‰ ì¿¼ë¦¬ ë³€í™˜ì„ ìœ„í•œ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸
        self.system_prompt = (
            "ë‹¹ì‹ ì€ ê²€ìƒ‰ ì¿¼ë¦¬ ìµœì í™” AIì…ë‹ˆë‹¤. "
            "ì‚¬ìš©ìì˜ ì§ˆë¬¸ì„ ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ê²€ìƒ‰í•˜ê¸° ì¢‹ì€ '1~3ê°œì˜ êµ¬ì²´ì ì¸ ì§ˆë¬¸'ìœ¼ë¡œ ë¶„í•´í•˜ê±°ë‚˜ ì¬ì‘ì„±í•˜ì„¸ìš”. "
            "ë¶ˆí•„ìš”í•œ ì‚¬ì¡± ì—†ì´ ì˜¤ì§ ì˜ˆì‹œì— ë‚˜ì™€ìˆëŠ” json í˜•ì‹ìœ¼ë¡œë§Œ ì¶œë ¥í•˜ì„¸ìš”."
            "ì¬ì‘ì„±ëœ ì¿¼ë¦¬ëŠ” ë¦¬ìŠ¤íŠ¸ í˜•ì‹ìœ¼ë¡œ ì‘ì„±í•˜ì„¸ìš”."
            ""
            "ì˜ˆì‹œ)"
            "{"
            "  \"original_query\": ì´ë²ˆ ëŒ€íšŒì˜ ìš°ìŠ¹ìì™€ ì €ì˜ ì°¨ì´ì ì´ ë­¡ë‹ˆê¹Œ?"
            "  \"rewritten_queries\": [\"ì´ ëŒ€íšŒì˜ ìš°ìŠ¹ìëŠ” ëˆ„êµ¬ì…ë‹ˆê¹Œ?\", \"ì´ ëŒ€íšŒì˜ ìš°ìŠ¹ìì˜ íŠ¹ì§•ì€ ë¬´ì—‡ì…ë‹ˆê¹Œ?\"]"
            "}"
        )

    def rewrite(self, user_query):
        """
        ì‚¬ìš©ì ì§ˆë¬¸ -> ê²€ìƒ‰ ì¿¼ë¦¬ ë¦¬ìŠ¤íŠ¸ ë³€í™˜
        """
        # 1. í”„ë¡¬í”„íŠ¸ êµ¬ì„±
        messages = [
            {"role": "system", "content": self.system_prompt},
            {"role": "user", "content": f"ì§ˆë¬¸: {user_query}\n\nê²€ìƒ‰ ì¿¼ë¦¬:"}
        ]
        
        prompt = self.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
        
        # 2. ì¶”ë¡ 
        outputs = self.llm.generate([prompt], self.sampling_params)
        raw_output = outputs[0].outputs[0].text.strip()

        # <think> íƒœê·¸ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ë‚´ë¶€ ì‚¬ê³  ê³¼ì •ê³¼ ìµœì¢… ë‹µë³€ì„ ë¶„ë¦¬
        thinking_content = ""
        final_response = raw_output

        if "</think>" in raw_output:
            parts = raw_output.split("</think>")
            thinking_content = parts[0].replace("<think>", "").strip()
            if len(parts) > 1:
                final_response = parts[1].strip()
            else:
                final_response = "" # ì‚¬ê³  ê³¼ì •ë§Œ ì¶œë ¥ëœ ê²½ìš° ì˜ˆì™¸ ì²˜ë¦¬

        # 3. ê²°ê³¼ íŒŒì‹±
        try:
            # JSON íŒŒì‹± ì‹œë„ (LLMì´ ê°€ë” í˜•ì‹ì„ ì–´ê¸¸ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ì˜ˆì™¸ì²˜ë¦¬)
            queries = ast.literal_eval(final_response)['rewritten_queries']
        except:
            # íŒŒì‹± ì‹¤íŒ¨ ì‹œ ì›ë³¸ ì¿¼ë¦¬ ì‚¬ìš©
            queries = [user_query]
            
        print(f"ğŸ”„ [Rewriter] ì›ë³¸: '{user_query}' -> ë³€í™˜: {queries}")
        return list(queries)


class QwenVLLMChatbotWithRAG:
    def __init__(self, 
                 model_name="Qwen/Qwen3-30B-A3B-FP8", 
                 llm=None, 
                 tokenizer=None, 
                 query_rewriter=None,
                 embedding_model=None,
                 collection=None,
                 system_instructions=None):
        """
        RAG(Retrieval-Augmented Generation) ê¸°ëŠ¥ì„ íƒ‘ì¬í•œ Qwen ì±—ë´‡ ì´ˆê¸°í™”
        """
        print(f"Initializing Chatbot with model: {model_name}...")
        
        # -------------------------------------------------------
        # [1. ìƒì„± ëª¨ë¸(Generator) ì„¤ì •] - LLM & Tokenizer
        # -------------------------------------------------------
        if llm:
            print(">> [Generator] Existing LLM instance detected. Using provided engine.")
            self.llm = llm
        else:
            print(">> [Generator] Loading new vLLM engine...")
            self.llm = LLM(
                model=model_name,
                tensor_parallel_size=1,
                gpu_memory_utilization=0.90,
                dtype="auto",
                trust_remote_code=True
            )

        if tokenizer:
            self.tokenizer = tokenizer
        else:
            self.tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)

        # -------------------------------------------------------
        # [2. RAG ì»´í¬ë„ŒíŠ¸ ì„¤ì •] - Rewriter, Embedder, Vector DB
        # -------------------------------------------------------
        
        if query_rewriter:
            print(">> [RAG] Existing Query Rewriter detected.")
            self.query_rewriter = query_rewriter
        else:
            print(">> [RAG] Loading new Query Rewriter...")
            self.query_rewriter = QueryRewriter(self.llm, self.tokenizer)

        if embedding_model:
            print(">> [RAG] Existing Embedding Model detected.")
            self.embedding_model = embedding_model
        else:
            print(">> [RAG] Loading new BGE-M3 Embedding Model...")
            self.embedding_model = BGEM3FlagModel('BAAI/bge-m3', use_fp16=True)

        if collection:
            print(">> [RAG] Existing ChromaDB Collection detected.")
            self.collection = collection
        else:
            print(">> [RAG] Connecting to ChromaDB...")
            # Streamlit ìºì‹œ ë¬¸ì œ ë°©ì§€ë¥¼ ìœ„í•´ ê²½ë¡œ í™•ì¸ í•„ìš”
            client = chromadb.PersistentClient(path="./chroma_db")
            self.collection = client.get_or_create_collection("persona_memory")
        
        self.system_instructions = system_instructions
        self.clear_history() 

    def search_relatives(self, query, top_k=3):
        print(f"\n   [Retrieval] ê²€ìƒ‰ ìˆ˜í–‰: '{query}'")
        start_time = time.time()
        
        try:
            query_embeddings = self.embedding_model.encode(query, batch_size=1)['dense_vecs']
            
            # 3. ê²°ê³¼ì—ì„œ ìš°ë¦¬ê°€ í•„ìš”í•œ ì²« ë²ˆì§¸(ì§„ì§œ ì§ˆë¬¸) ê²°ê³¼ë§Œ ê°€ì ¸ì˜´
            query_embedding = query_embeddings[0]
            
        except Exception as e:
            print(f"   [Error] ì„ë² ë”© ìš°íšŒ ì²˜ë¦¬ ì¤‘ ì¹˜ëª…ì  ì˜¤ë¥˜: {e}")
            # ìµœì•…ì˜ ê²½ìš°ë¥¼ ëŒ€ë¹„í•œ 2ì°¨ ë°©ì–´ì„  (ë¹„íš¨ìœ¨ì ì´ì§€ë§Œ ì‘ë™ì€ í•˜ê²Œ í•¨)
            # ì—¬ê¸°ì„œëŠ” ì–´ì©” ìˆ˜ ì—†ì´ ì—ëŸ¬ê°€ ë‚˜ë”ë¼ë„ ì§„í–‰ë˜ë„ë¡ ì˜ˆì™¸ ì²˜ë¦¬
            return {"ids": [[]], "documents": [[]], "distances": [[]]}

        results = self.collection.query(
            query_embeddings=[query_embedding.tolist()],
            n_results=top_k
        )
        
        search_time = time.time() - start_time
        print(f"   ê²€ìƒ‰ ì†Œìš” ì‹œê°„: {search_time:.3f}ì´ˆ")
        return results

    def synthesize_input_with_contexts(self, user_input, contexts):
        reformatted_contexts = ""
        for context in contexts:
            reformatted_contexts += f"- {context}\n            "
        
        template = f"""
        <reference_documents>
            {reformatted_contexts}
        </reference_documents>

        ì‚¬ìš©ì ì§ˆë¬¸: {user_input}
        
        ìœ„ <reference_documents>ì˜ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ë‹µë³€í•˜ì„¸ìš”.
        """
        return template
        
    def generate_response(self, user_input):
        """
        [Main Flow] RAG íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ë° ë‹µë³€ ìƒì„±
        """
        print("##### [RAG Pipeline Start] #####")
        print(f"1. ìœ ì € ì›ë³¸ ì…ë ¥: {user_input}")
        
        # Step 1: Rewrite
        print("\n### Step 1: Query Rewriting ###")
        queries_for_rag = self.query_rewriter.rewrite(user_input)
        print(f"-> ì¬ì‘ì„±ëœ ì¿¼ë¦¬ ëª©ë¡: {queries_for_rag}")

        # Step 2: Retrieve
        print("\n### Step 2: Information Retrieval ###")
        retrieved_results = []
        unique_docs = set()

        for query in queries_for_rag:
            relatives = self.search_relatives(query)
            for idx, (doc_id, document, distance) in enumerate(zip(relatives['ids'][0], relatives['documents'][0], relatives['distances'][0])):
                if document not in unique_docs:
                    print(f"   ë¬¸ì„œ ë°œê²¬ (ID: {doc_id}, Dist: {distance:.4f}): {document}")
                    retrieved_results.append(document)
                    unique_docs.add(document)

        # Step 3: Synthesize
        print("\n### Step 3: Context Synthesis ###")
        synthesized_input = self.synthesize_input_with_contexts(user_input, retrieved_results)
        print(f"-> ìµœì¢… í”„ë¡¬í”„íŠ¸ ê¸¸ì´: {len(synthesized_input)}ì ìƒì„±ë¨")
        
        # Step 4: Generate
        print("\n### Step 4: LLM Generation ###")
        current_messages = self.history + [{"role": "user", "content": synthesized_input}]
        
        prompt_str = self.tokenizer.apply_chat_template(
            current_messages,
            tokenize=False,
            add_generation_prompt=True
        )

        sampling_params = SamplingParams(
            temperature=0.7,
            top_p=0.8,
            max_tokens=2048,
            repetition_penalty=1.05,
        )

        outputs = self.llm.generate([prompt_str], sampling_params)
        raw_output = outputs[0].outputs[0].text.strip()

        # Step 5: Parsing
        thinking_content = ""
        final_response = raw_output

        if "</think>" in raw_output:
            parts = raw_output.split("</think>")
            thinking_content = parts[0].replace("<think>", "").strip()
            if len(parts) > 1:
                final_response = parts[1].strip()
            else:
                final_response = ""

        # History Update (Clean version)
        self.history.append({"role": "user", "content": user_input})
        self.history.append({"role": "assistant", "content": final_response})

        # =======================================================
        # [ìˆ˜ì •ëœ ë¶€ë¶„] Streamlit UIì— ë§ê²Œ Return ê°’ êµ¬ì¡° ë³€ê²½
        # =======================================================
        now_str = datetime.now().strftime("%p %I:%M").replace("AM", "ì˜¤ì „").replace("PM", "ì˜¤í›„")
        
        return {
            "response": final_response,      # ì‹¤ì œ ë‹µë³€
            "timestamp": now_str,            # íƒ€ì„ìŠ¤íƒ¬í”„
            "details": {
                "thought": thinking_content,     # ì‚¬ê³  ê³¼ì •
                "rewritten_queries": queries_for_rag, # RAGìš© ì¬ì‘ì„± ì¿¼ë¦¬
                "retrieved_docs": retrieved_results   # RAGìš© ê²€ìƒ‰ ë¬¸ì„œ
            },
            "raw": raw_output                # ì›ë³¸ (ë””ë²„ê¹…ìš©)
        }
        
    def clear_history(self):
        if self.system_instructions:
            self.history = [{"role": "system", "content": self.system_instructions}]
        else:
            self.history = []
        print(">> Chat history cleared.")

# ==========================================
# 2. Streamlit UI Logic
# ==========================================

# CSS ë¡œë“œ í•¨ìˆ˜
def load_css():
    st.markdown("""
    <style>
        .chat-container { display: flex; flex-direction: column; gap: 10px; padding: 10px; }
        .chat-bubble { max-width: 70%; padding: 12px 16px; border-radius: 15px; position: relative; font-size: 16px; line-height: 1.5; box-shadow: 0 1px 2px rgba(0,0,0,0.1); margin-bottom: 5px; }
        .bot-row { display: flex; justify-content: flex-start; align-items: flex-end; margin-bottom: 10px; }
        .bot-bubble { background-color: #F2F2F2; color: #000000; border-top-left-radius: 2px; }
        .user-row { display: flex; justify-content: flex-end; align-items: flex-end; margin-bottom: 10px; }
        .user-bubble { background-color: #FEE500; color: #000000; border-top-right-radius: 2px; }
        .timestamp { font-size: 10px; color: #888888; margin: 0 5px; min-width: 40px; }
        .streamlit-expanderHeader { font-size: 14px; color: #555; background-color: #fafafa; border-radius: 5px; }
    </style>
    """, unsafe_allow_html=True)

# í˜ì´ì§€ ì„¤ì •
st.set_page_config(page_title="ë‚˜ë§Œì˜ í˜ë¥´ì†Œë‚˜ ì±—ë´‡ ë©”ì‹ ì €", layout="centered")
load_css()

# ì´ˆê¸° í™˜ì˜ ë©”ì‹œì§€ ì •ì˜
initial_message = {
    "role": "assistant",
    "content": "ì—¬ëŸ¬ë¶„ì˜ í˜ë¥´ì†Œë‚˜ì™€ ì´ì•¼ê¸°í•´ë³´ì„¸ìš”!",
    "timestamp": datetime.now().strftime("%p %I:%M").replace("AM", "ì˜¤ì „").replace("PM", "ì˜¤í›„"),
    "details": None
}

# ------------------------------------------------
# [ì¤‘ìš”] ë´‡ ì´ˆê¸°í™” (vLLM ë¡œë”©ì€ ì„¸ì…˜ë‹¹ 1íšŒë§Œ ìˆ˜í–‰)
# ------------------------------------------------
if "bot" not in st.session_state:
    with st.spinner("AI ëª¨ë¸ì„ ë¡œë”© ì¤‘ì…ë‹ˆë‹¤... (ì‹œê°„ì´ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤)"):
        # ì‹¤ì œ ë´‡ í´ë˜ìŠ¤ ì¸ìŠ¤í„´ìŠ¤í™”
        # ì£¼ì˜: system_instructions ë“±ì€ í•„ìš”ì— ë”°ë¼ ìˆ˜ì •í•˜ì„¸ìš”.
        st.session_state.bot = QwenVLLMChatbotWithRAG(
            system_instructions=instructions.ahn_sungjae
        )

# ëŒ€í™” ê¸°ë¡ ì´ˆê¸°í™”
if "chat_history" not in st.session_state:
    st.session_state.chat_history = [initial_message]

st.title("Persona AI Chatbot")

# ------------------------------------------------
# Sidebar: ì„¤ì • ë° ì´ˆê¸°í™”
# ------------------------------------------------
with st.sidebar:
    if st.button("ëŒ€í™” ë‚´ìš© ì´ˆê¸°í™”", type="primary"):
        # ë´‡ ë‚´ë¶€ íˆìŠ¤í† ë¦¬ë„ ì´ˆê¸°í™” í•„ìš”
        st.session_state.bot.clear_history()
        # í™”ë©´ìš© íˆìŠ¤í† ë¦¬ ë¦¬ì…‹
        st.session_state.chat_history = [initial_message]
        st.rerun()

# ------------------------------------------------
# UI ë Œë”ë§: ì±„íŒ… ê¸°ë¡ ì¶œë ¥
# ------------------------------------------------
chat_container = st.container()

with chat_container:
    for msg in st.session_state.chat_history:
        if msg["role"] == "user":
            # ìœ ì € ë©”ì‹œì§€
            st.markdown(f"""
            <div class="user-row">
                <span class="timestamp">{msg['timestamp']}</span>
                <div class="chat-bubble user-bubble">
                    {msg['content']}
                </div>
            </div>
            """, unsafe_allow_html=True)
            
        else:
            # ë´‡ ë©”ì‹œì§€
            st.markdown(f"""
            <div class="bot-row">
                <div class="chat-bubble bot-bubble">
                    {msg['content']}
                </div>
                <span class="timestamp">{msg['timestamp']}</span>
            </div>
            """, unsafe_allow_html=True)
            
            # [ìƒì„¸ ë³´ê¸° Expander] - RAG ì •ë³´ í‘œì‹œ
            if msg.get("details"):
                with st.expander("ğŸ” AI ì‚¬ê³  ê³¼ì • ë° ê·¼ê±° ë³´ê¸°"):
                    # 1. ì¿¼ë¦¬ ì¬êµ¬ì„±
                    if msg['details'].get('rewritten_queries'):
                        st.markdown("**1. ì§ˆë¬¸ ì¬êµ¬ì„± (Rewriting)**")
                        for q in msg['details']['rewritten_queries']:
                            st.code(q, language='text')

                    # 2. ì°¸ê³  ë¬¸ì„œ
                    if msg['details'].get('retrieved_docs'):
                        st.markdown("**2. ì°¸ê³  ë¬¸ì„œ (Context)**")
                        for doc in msg['details']['retrieved_docs']:
                            st.success(doc)

                    # 3. ì‚¬ê³  ê³¼ì •
                    if msg['details'].get('thought'):
                        st.markdown("**3. ì‚¬ê³  ê³¼ì • (Thinking)**")
                        st.info(msg['details']['thought'])

# ------------------------------------------------
# ì…ë ¥ ì²˜ë¦¬
# ------------------------------------------------
if prompt := st.chat_input("ë©”ì‹œì§€ë¥¼ ì…ë ¥í•˜ì„¸ìš”..."):
    # 1. ìœ ì € ë©”ì‹œì§€ ì¦‰ì‹œ ì¶”ê°€
    now_time = datetime.now().strftime("%p %I:%M").replace("AM", "ì˜¤ì „").replace("PM", "ì˜¤í›„")
    st.session_state.chat_history.append({
        "role": "user",
        "content": prompt,
        "timestamp": now_time,
        "details": None
    })
    st.rerun() 

# ------------------------------------------------
# ë‹µë³€ ìƒì„±
# ------------------------------------------------
if st.session_state.chat_history[-1]["role"] == "user":
    with st.spinner("ìƒê°í•˜ëŠ” ì¤‘..."):
        # ì‹¤ì œ ë´‡ ë¡œì§ ì‹¤í–‰
        result = st.session_state.bot.generate_response(st.session_state.chat_history[-1]["content"])
        
        # ê²°ê³¼ ì €ì¥
        st.session_state.chat_history.append({
            "role": "assistant",
            "content": result["response"],
            "timestamp": result["timestamp"],
            "details": result["details"]
        })
        st.rerun()
